{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods utilize multiple models, typically called 'weak learners' as individually these models often do not achieve optimal performance, to create a single overall model that often has much better performance, in terms of either bias and variance, than the individual models in isolation. Ensemble methods are famously applied using tree based models (random forests and XGBoost) but those are by no means the only models utilized by all ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, short for bootstrap aggregating, utilizes the insight of [bootstrapping](../../statistics/classic-stats/bootstrap.ipynb). It proceeds by creating $B$ bootstrapped datasets from the original dataset. This is done, again as in the bootstrap scenario, by sampling data points from the original dataset _with replacement_ to generate each of the $B$ datasets. We then train a separate weak learner on data set $b$ leading to a fit model $\\hat{f}^{*b}(x)$. We then take these models and average their predicitions to get our final prediction for a new sample \n",
    "\n",
    "$$\n",
    "\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B\\hat{f}^{*b}(x).\n",
    "$$\n",
    "\n",
    "For regression problems we can take a simple average of the prediction and for classification models we can simply take the majority class predicted by the ensemble of models (although there are other ways of selecting a final prediction). This method drastically reduces the variance that usually accompanies using a single decision tree and in fact increasing the number of trees $B$ used will not lead to overfitting due to the use of bootstrapping to create the datasets each tree is fit to which leads to a reduction in variance in the predictions made by the model {cite:p}`james2023statistical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While bagging reduces the variances of the model via the use of an ensemble the resulting trees can be somewhat correlated as they may all find that the same subset of features are the most informative when learning the function we want to model. We can further de-correlate the trees in the ensemble by using the random forest method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
