{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection & assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is the process of selecting the proper level of flexibility for a model (tuning hyperparameters for example) {cite:p}`james2023statistical` or equivalently the process of estimating the performance of different models in order to choose the best one {cite:p}`hastie2009elements`. Model assessment is the process of estimating the test/generalization error of the _selected_ model on new/unseen data {cite:p}`hastie2009elements`. Note that we cannot use the training error rate to estimate the test error rate as the former often can dramatically underestimate the later {cite:p}`james2023statistical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Test Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When one has ample amounts of training data the best option is usually to randomly split the data into training, validation and testing sets {cite:p}`hastie2009elements`. The training set is used to train/fit the model, the validation set is used to estimate the prediction/generalization error for model selection and hyperparameter tuning and the test set (also called a hold-out set) is treated as a 'hold out' set which is used to evaluate the performance (generalization error) of the *selected* model. Often times however we are not in this data rich regime and collecting more data is prohibitively expensive so alternative methods must be used for model selection. Three such methods are discussed below that are also discussed in the chapter _Resampling Methods_ of {cite:p}`james2023statistical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of enough data to form training, validation and testing sets one could _randomly_ divide the data into only training and validation sets. We then fit the model to the training set, which typically contains around $50\\%-60\\%$ of the observations in the original dataset, and use the fitted models' error rate on the validation set (validation error rate) as _an estimate of the test error rate_ {cite:p}`james2023statistical`. There are two drawbacks with this method.\n",
    "\n",
    "1. The resulting validation errors can have high variance, as shown in Figure 5.2 of {cite:p}`james2023statistical`, depending on which samples are in the training and the validation set.\n",
    "\n",
    "2. This method can _overestimate_ the test error of a model that is fit to the whole dataset since we are using a substantial amount of the data in the validation set instead of using that data for training and statistical methods tend to perform worse when trained on fewer samples than more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-One-Out Cross-Validation (LOOCV) addresses some of the drawbacks of the validation set approach. With LOOCV we take the original dataset of size $n$ and create a training set of size $n-1$ and a validation set of size $1$. We then proceed as we did for the validation set approach by fitting the model to the training set and computing the validation error rate, denoted $\\textrm{VER}_1$ here, on the validation set. Clearly the variance of $\\textrm{VER}_1$ will be large since we only have one sample in our validation set {cite:p}`james2023statistical`. So what we will do instead is repeat this process $n$ times each time using a different sample for the validation set. We will then have $n$ values for the validation error which we can average to get\n",
    "\n",
    "$$\n",
    "\\textrm{CV}_{(n)} = \\frac{1}{n}\\sum_{i=1}^{n} \\textrm{VER}_i\n",
    "$$\n",
    "\n",
    "which is the LOOCV estimate of the _test_ error rate.\n",
    "LOOCV has two advantages over the validation set approach. \n",
    "\n",
    "1. It's estimate of the test error has less bias since we use $n-1$ samples to fit the model unlike the validation set approach which typically uses only $\\approx50\\%$ of the available data for training. Therefore, LOOCV tends to not overestimate the test error like the validation set approach does.\n",
    "\n",
    "2. Unlike the validation set approach no randomness is involved in LOOCV as we simply leave exactly one sample out of the training set at each step. Thus LOOCV always gives the same result for $\\textrm{CV}_{(n)}$ given a particular dataset and model.\n",
    "\n",
    "\n",
    "Although this method does come with a time complexity of $O(n)$ as we have to fit the model $n$ times unlike the validation set approach where we just need to fit the model once i.e. it has a time complexity $O(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative to the validation set approach is $k$-fold cross validation ($k$-fold CV). In this method we split the dataset into $k$ groups (folds) of roughly equal size. We use the data in $k-1$ of the groups as the training set and the data in remaining group as our validation set. We do this $k$ times which gives us $k$ values of the validation error rate $\\textrm{VER}_1, \\textrm{VER}_2,\\dots, \\textrm{VER}_k$.\n",
    "We will then average these values to get\n",
    "\n",
    "$$\n",
    "\\textrm{CV}_{(k)} = \\frac{1}{k}\\sum_{i=1}^{k} \\textrm{VER}_i\n",
    "$$\n",
    "\n",
    "which is the $k$-fold CV estimate of the _test_ error rate. Clearly LOOCV is a special case of $k$-fold CV in which $k=n$. The advatange of this method over LOOCV is that it's complexity is $O(k)$ as we only need to fit the model $k$ times and $k$ is typically taken to be $5$ or $10$ this is extremely useful for models which may have high complexity to train, such as neural networks or SVMs, as our model selection strategies time complexity no longer scales linearly with the dataset size but is in fact constant in the size of the dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
