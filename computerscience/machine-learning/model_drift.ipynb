{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6ef61e",
   "metadata": {},
   "source": [
    "# Model drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac4dc7",
   "metadata": {},
   "source": [
    "## Covariate Shift: $P(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b556b77",
   "metadata": {},
   "source": [
    "This refers to a change in $P(X)$ with $P(Y|X)$ remaining fixed i.e. the distribution of the input data/features changes but the conditional distribution over outputs/labels given this new distribution stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61543",
   "metadata": {},
   "source": [
    "## Label Shift: $P(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff5ed9",
   "metadata": {},
   "source": [
    "This refers to a change in $P(Y)$ with $P(X|Y)$ remaining fixed i.e. the distribution over outputs/labels changes but the conditional distribution over the input data/features given this new distribution stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161d3bb",
   "metadata": {},
   "source": [
    "## Concept Drift: $P(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f60037",
   "metadata": {},
   "source": [
    "This refers to a change in $P(Y|X)$ with $P(X)$ remaining fixed i.e. the conditional distribution over outputs/labels changes given the input/features but the distribution of input data/features stays the same. This can be interpreted as a change occurring in the relationship between the input data/features and the output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91477c1",
   "metadata": {},
   "source": [
    "## Drift Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739f481",
   "metadata": {},
   "source": [
    "Drift detection methods were proposed in {cite:p}`rabanser2019failing` for detecting covariate drift in high-dimensional data given data from a source distribution and a target distributions. Essentially the process involves using a dimensionality reduction technique e.g. PCA, sparse random projections, autoencoders etc, to reduce the dimensionality of the data to $D$ which can be very large to a lower dimensionality $K$ where $K \\ll D$. Afterwards the resulting $K$-dimensional representations of the data from the two distributions are tested using a hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68062c8d",
   "metadata": {},
   "source": [
    "One approach taken in the paper is to use a Kolmogorov-Smirnov (KS) Test. Since the KS test is only defined for univarite random variables they perform a KS test for each of the $K$-dimensions separately.  This results in $K$ separate $p$ values which they then use for significance testing by use of the Bonferroni correction. This is a very conservative correction which rejects the null hypothesis if the minimum $p$-value\n",
    "among all tests is less than $\\frac{\\alpha}{K}$ (where $\\alpha$ (typically $0.05$) is the significance level of the test). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9d5ae",
   "metadata": {},
   "source": [
    "````{prf:remark} Bonferroni correction\n",
    ":class: dropdown\n",
    "\n",
    "The Bonferroni correction is an adjustment used in multiple hypothesis testing \n",
    "\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
