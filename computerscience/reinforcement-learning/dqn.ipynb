{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "torch.manual_seed(2022)  # set random seeds for reproducibility \n",
    "random.seed(2022)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: list[int],\n",
    "        output_dim: int,\n",
    "        activation_fn: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(hidden_dims) > 0, \"Must have at least one hidden layer.\"\n",
    "\n",
    "        # add input layer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(in_features=input_dim, out_features=hidden_dims[0])]\n",
    "        )\n",
    "        self.layers.append(activation_fn())\n",
    "\n",
    "        # add hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.layers.append(\n",
    "                nn.Linear(in_features=hidden_dims[i], out_features=hidden_dims[i+1])\n",
    "            )\n",
    "            self.layers.append(activation_fn())\n",
    "\n",
    "        # add output layer\n",
    "        self.layers.append(\n",
    "            nn.Linear(in_features=hidden_dims[-1], out_features=output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "\n",
    "        return input\n",
    "\n",
    "\n",
    "class DQNTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qnet: nn.Module,\n",
    "        num_actions: int,\n",
    "        buffer_size: int,\n",
    "        batch_size: int,\n",
    "        gamma: float,\n",
    "        epsilon_decay: float,\n",
    "        epsilon_min: float,\n",
    "        network_update_interval: int,\n",
    "        learning_rate: float\n",
    "    ):\n",
    "        # initialize replay buffer/memory\n",
    "        self.replay_buffer = deque([], maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.qnet = qnet\n",
    "        self.target_qnet = deepcopy(qnet)\n",
    "        self.optimizer = optim.Adam(self.qnet.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.num_actions = num_actions\n",
    "        self.network_update_interval = network_update_interval\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> int:\n",
    "        if self.epsilon > random.uniform(0, 1):\n",
    "            # select random action from [0,1,...,num_actions]\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            Q_values = self.qnet(state)\n",
    "            action = torch.argmax(Q_values).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def compute_loss(self, minibacth: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        loss = 0\n",
    "        for item, label in zip(minibacth, labels):\n",
    "            state = item[0]\n",
    "            action = item[1]\n",
    "            pred = self.qnet(state)[action]\n",
    "            loss += self.loss_func(pred, label) / len(minibacth)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def get_labels(self, minibacth: list[tuple[Tensor, int, float, Tensor, bool]]) -> torch.Tensor:\n",
    "        labels = []\n",
    "        for memory in minibacth:\n",
    "            state = memory[0]\n",
    "            action = memory[1]\n",
    "            reward = memory[2]\n",
    "            next_state = memory[3]\n",
    "            next_state_is_terminal = memory[4]\n",
    "            if next_state_is_terminal:\n",
    "                labels.append(reward)\n",
    "            else:\n",
    "                labels.append( reward + self.gamma*torch.max(self.target_qnet(next_state)).item() )\n",
    "\n",
    "        return torch.tensor(labels)\n",
    "\n",
    "    def train(self, num_episodes: int, max_timesteps: int):\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "        losses = []\n",
    "        episode_rewards = [0]*num_episodes\n",
    "\n",
    "        for m in range(num_episodes):\n",
    "            # reset env at start of episode and get starting state\n",
    "            state, _ = env.reset()\n",
    "\n",
    "            for t in range(max_timesteps):\n",
    "                action = self.select_action(torch.tensor(state))\n",
    "                \n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "                self.replay_buffer.append( (torch.tensor(state), action, float(reward), torch.tensor(new_state), terminated) )\n",
    "                episode_rewards[m] += reward\n",
    "\n",
    "\n",
    "\n",
    "                if len(self.replay_buffer) >= self.batch_size:\n",
    "                    # sample uniformly without replacement\n",
    "                    minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
    "                    labels = self.get_labels(minibatch)\n",
    "                    \n",
    "                    loss = self.compute_loss(minibatch, labels)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    if t % self.network_update_interval == 0:\n",
    "                        qnet_state_dict = self.qnet.state_dict()\n",
    "                        self.target_qnet.load_state_dict(qnet_state_dict)  # \\hat{Q} = Q\n",
    "                \n",
    "                state = new_state\n",
    "                \n",
    "                if terminated:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = min(self.epsilon*(1-self.epsilon_decay), self.epsilon_min)\n",
    "\n",
    "        return losses, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = DeepQNetwork(4, [50, 50], 2, nn.ReLU)\n",
    "\n",
    "trainer = DQNTrainer(\n",
    "    qnet,\n",
    "    num_actions=2,\n",
    "    buffer_size=75000,\n",
    "    batch_size=256,\n",
    "    gamma=0.99,\n",
    "    epsilon_decay=0.009,\n",
    "    epsilon_min=0.005,\n",
    "    network_update_interval=250,\n",
    "    learning_rate=0.0005,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, episode_rewards = trainer.train(num_episodes=150, max_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.yscale(\"log\")\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
